{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget https://github.com/stefan-it/nmt-en-vi/raw/master/data/train-en-vi.tgz\n",
    "# !tar -zxf train-en-vi.tgz\n",
    "# !wget https://github.com/stefan-it/nmt-en-vi/raw/master/data/dev-2012-en-vi.tgz\n",
    "# !tar -zxf dev-2012-en-vi.tgz\n",
    "# !wget https://github.com/stefan-it/nmt-en-vi/raw/master/data/test-2013-en-vi.tgz\n",
    "# !tar -zxf test-2013-en-vi.tgz\n",
    "\n",
    "# !rm train-en-vi.tgz dev-2012-en-vi.tgz test-2013-en-vi.tgz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import malaya\n",
    "import re\n",
    "import os\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = malaya.preprocessing.SocialTokenizer().tokenize\n",
    "\n",
    "def is_number_regex(s):\n",
    "    if re.match(\"^\\d+?\\.\\d+?$\", s) is None:\n",
    "        return s.isdigit()\n",
    "    return True\n",
    "\n",
    "def tokenizing(string):\n",
    "    tokenized = tokenizer(string)\n",
    "    tokenized = ['<NUM>' if is_number_regex(w) else w for w in tokenized]\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "db_dir = os.getcwd()\n",
    "\n",
    "train_en = []\n",
    "train_vi = []\n",
    "\n",
    "test_en = []\n",
    "test_vi = []\n",
    "\n",
    "with open('train.en', 'r') as f_train_en, open('train.vi', 'r') as f_train_vi:\n",
    "    train_en.extend(f_train_en.read().split('\\n')[:-1])\n",
    "    train_vi.extend(f_train_vi.read().split('\\n')[:-1])\n",
    "\n",
    "with open('tst2012.en', 'r') as f_test_en, open('tst2012.vi', 'r') as f_test_vi:\n",
    "    test_en.extend(f_test_en.read().split('\\n')[:-1])\n",
    "    test_vi.extend(f_test_vi.read().split('\\n')[:-1])\n",
    "\n",
    "with open('tst2013.en', 'r') as f_test_en, open('tst2013.vi', 'r') as f_test_vi:\n",
    "    test_en.extend(f_test_en.read().split('\\n')[:-1])\n",
    "    test_vi.extend(f_test_vi.read().split('\\n')[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 133317/133317 [00:46<00:00, 2877.75it/s]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(len(train_en))):\n",
    "    tokenized_en = ' '.join(tokenizing(train_en[i]))\n",
    "    tokenized_vi = ' '.join(tokenizing(train_vi[i]))\n",
    "    train_en[i] = tokenized_en\n",
    "    train_vi[i] = tokenized_vi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2821/2821 [00:00<00:00, 2968.76it/s]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(len(test_en))):\n",
    "    tokenized_en = ' '.join(tokenizing(test_en[i]))\n",
    "    tokenized_vi = ' '.join(tokenizing(test_vi[i]))\n",
    "    test_en[i] = tokenized_en\n",
    "    test_vi[i] = tokenized_vi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "import json\n",
    "\n",
    "def build_dataset(words, n_words, atleast=1):\n",
    "    count = [['PAD', 0], ['GO', 1], ['EOS', 2], ['UNK', 3]]\n",
    "    counter = collections.Counter(words).most_common(n_words)\n",
    "    counter = [i for i in counter if i[1] >= atleast]\n",
    "    count.extend(counter)\n",
    "    dictionary = dict()\n",
    "    for word, _ in count:\n",
    "        dictionary[word] = len(dictionary)\n",
    "    data = list()\n",
    "    unk_count = 0\n",
    "    for word in words:\n",
    "        index = dictionary.get(word, 0)\n",
    "        if index == 0:\n",
    "            unk_count += 1\n",
    "        data.append(index)\n",
    "    count[0][1] = unk_count\n",
    "    reversed_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "    return data, count, dictionary, reversed_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab from size: 48111\n",
      "Most common words [(',', 156175), ('.', 135024), ('the', 103138), ('to', 65797), (\"'\", 64433), ('of', 60341)]\n",
      "Sample data [6531, 16858, 55, 58, 335, 593, 11, 731, 5477, 132] ['Rachel', 'Pike', ':', 'The', 'science', 'behind', 'a', 'climate', 'headline', 'In']\n"
     ]
    }
   ],
   "source": [
    "concat_from = ' '.join(train_en).split()\n",
    "vocabulary_size_from = len(list(set(concat_from)))\n",
    "data_from, count_from, dictionary_from, rev_dictionary_from = build_dataset(concat_from, vocabulary_size_from)\n",
    "print('Vocab from size: %d'%(vocabulary_size_from))\n",
    "print('Most common words', count_from[4:10])\n",
    "print('Sample data', data_from[:10], [rev_dictionary_from[i] for i in data_from[:10]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab to size: 22465\n",
      "Most common words [(',', 128672), ('.', 125418), ('là', 58046), ('tôi', 52058), ('một', 49025), ('có', 48322)]\n",
      "Sample data [1915, 66, 1136, 128, 8, 372, 111, 38, 412, 724] ['Khoa', 'học', 'đằng', 'sau', 'một', 'tiêu', 'đề', 'về', 'khí', 'hậu']\n"
     ]
    }
   ],
   "source": [
    "concat_to = ' '.join(train_vi).split()\n",
    "vocabulary_size_to = len(list(set(concat_to)))\n",
    "data_to, count_to, dictionary_to, rev_dictionary_to = build_dataset(concat_to, vocabulary_size_to)\n",
    "print('Vocab to size: %d'%(vocabulary_size_to))\n",
    "print('Most common words', count_to[4:10])\n",
    "print('Sample data', data_to[:10], [rev_dictionary_to[i] for i in data_to[:10]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "with open('train-test.json', 'w') as fopen:\n",
    "    json.dump({'train_X': train_en, 'train_Y': train_vi,\n",
    "              'test_X': test_en,\n",
    "              'test_Y': test_vi}, fopen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dictionary.json', 'w') as fopen:\n",
    "    json.dump({'from': {'dictionary': dictionary_from, 'rev_dictionary': rev_dictionary_from},\n",
    "              'to': {'dictionary': dictionary_to, 'rev_dictionary': rev_dictionary_to}}, fopen)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

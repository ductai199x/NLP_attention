{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.layers import Input, Dense, Embedding, Lambda, TimeDistributed, \\\n",
    "                                    Add, Conv1D, Dropout, Concatenate, Activation\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "\n",
    "from model import create_model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "physical_devices = tf.config.experimental.list_physical_devices('GPU') \n",
    "for physical_device in physical_devices: \n",
    "    tf.config.experimental.set_memory_growth(physical_device, True)\n",
    "\n",
    "print(physical_devices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "from tqdm.auto import tqdm\n",
    "import time\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/sweet/1-workdir/nlp_attention/en_vi_attention_nlp/src\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layers=2\n",
    "num_multi_heads=3\n",
    "d_k=64\n",
    "d_v=64\n",
    "d_model=256\n",
    "optimizer=\"adam\"\n",
    "null_token_value=0\n",
    "source_vocab_size = 48114\n",
    "target_vocab_size = 22468\n",
    "share_word_embedding=False\n",
    "MAXIMUM_TEXT_LENGTH = 866"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.experimental.AUTOTUNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_records=\"/home/sweet/1-workdir/nlp_attention/en_vi_attention_nlp/src/train.tfrecords\"\n",
    "validating_records=\"/home/sweet/1-workdir/nlp_attention/en_vi_attention_nlp/src/test.tfrecords\"\n",
    "\n",
    "raw_training_set = tf.data.TFRecordDataset(training_records)\n",
    "raw_validating_set = tf.data.TFRecordDataset(validating_records)\n",
    "\n",
    "\n",
    "feature_description = {\n",
    "    'input': tf.io.FixedLenFeature([], tf.string),\n",
    "    'target': tf.io.FixedLenFeature([], tf.string)\n",
    "}\n",
    "\n",
    "def _parse_record_function(example_proto):\n",
    "    # Parse the input tf.Example proto using the dictionary above.\n",
    "    features = tf.io.parse_single_example(example_proto, feature_description)\n",
    "    X = tf.io.decode_raw(features['input'], np.int32)\n",
    "    Y = tf.io.decode_raw(features['target'], np.int32)\n",
    "    return X[3:], Y[3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('/cpu:0'):\n",
    "    BUFFER_SIZE_TRAIN = 133317\n",
    "    BATCH_SIZE_TRAIN = 32\n",
    "    N_STEPS_PER_EPOCH_TRAIN = int(np.ceil(BUFFER_SIZE_TRAIN/BATCH_SIZE_TRAIN))\n",
    "\n",
    "    train_dataset = raw_training_set.map(_parse_record_function)\\\n",
    "                                    .repeat()\\\n",
    "                                    .batch(batch_size=BATCH_SIZE_TRAIN)\\\n",
    "                                    .prefetch(buffer_size=AUTOTUNE)\n",
    "    train_gen = train_dataset.__iter__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "with tf.device('/cpu:0'):\n",
    "    BUFFER_SIZE_VAL = 2821\n",
    "    BATCH_SIZE_VAL = 32\n",
    "    N_STEPS_PER_EPOCH_VAL = int(np.ceil(BUFFER_SIZE_VAL/BATCH_SIZE_VAL))\n",
    "\n",
    "    validation_dataset = raw_validating_set.map(_parse_record_function)\\\n",
    "                                        .batch(batch_size=BATCH_SIZE_VAL)\\\n",
    "                                        .prefetch(buffer_size=AUTOTUNE)\n",
    "    val_gen = validation_dataset.__iter__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([32, 865]), TensorShape([32, 865]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_input_batch, example_target_batch = next(train_gen)\n",
    "example_input_batch.shape, example_target_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with tf.device('/cpu:0'):\n",
    "    training_model, inference_model = create_model(source_vocab_size, target_vocab_size, \n",
    "                                                   MAXIMUM_TEXT_LENGTH,\n",
    "                                                   n=num_layers, d_model=256, h=num_multi_heads, \n",
    "                                                   optimizer=Adam(0.001, 0.9, 0.98, epsilon=1e-9))\n",
    "#     training_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e7751df358d4adfa02487fc60d9969e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4167.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 1ms/step - loss: 10.0183\n",
      "1/1 [==============================] - 0s 902us/step - loss: 9.7923\n",
      "1/1 [==============================] - 0s 807us/step - loss: 9.5825\n",
      "1/1 [==============================] - 0s 787us/step - loss: 9.3615\n",
      "1/1 [==============================] - 0s 899us/step - loss: 9.0832\n",
      "1/1 [==============================] - 0s 731us/step - loss: 8.5758\n",
      "1/1 [==============================] - 0s 707us/step - loss: 8.3769\n",
      "1/1 [==============================] - 0s 927us/step - loss: 8.0226\n",
      "1/1 [==============================] - 0s 943us/step - loss: 8.1062\n",
      "1/1 [==============================] - 0s 806us/step - loss: 7.6973\n",
      "1/1 [==============================] - 0s 731us/step - loss: 7.5782\n",
      "1/1 [==============================] - 0s 782us/step - loss: 6.9454\n",
      "1/1 [==============================] - 0s 970us/step - loss: 7.2686\n",
      "1/1 [==============================] - 0s 719us/step - loss: 7.0422\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 6.7930\n",
      "1/1 [==============================] - 0s 695us/step - loss: 6.7663\n",
      "1/1 [==============================] - 0s 841us/step - loss: 6.8564\n",
      "1/1 [==============================] - 0s 733us/step - loss: 6.4085\n",
      "1/1 [==============================] - 0s 803us/step - loss: 6.4275\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 6.4358\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 6.4282\n",
      "1/1 [==============================] - 0s 694us/step - loss: 6.4891\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 7.0260\n",
      "1/1 [==============================] - 0s 846us/step - loss: 6.8033\n",
      "1/1 [==============================] - 0s 728us/step - loss: 6.8724\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 6.4848\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 6.7684\n",
      "1/1 [==============================] - 0s 701us/step - loss: 6.3063\n",
      "1/1 [==============================] - 0s 995us/step - loss: 6.2269\n",
      "1/1 [==============================] - 0s 697us/step - loss: 6.3212\n",
      "1/1 [==============================] - 0s 919us/step - loss: 6.3896\n",
      "1/1 [==============================] - 0s 691us/step - loss: 6.2668\n",
      "1/1 [==============================] - 0s 987us/step - loss: 6.3789\n",
      "1/1 [==============================] - 0s 738us/step - loss: 6.2548\n",
      "1/1 [==============================] - 0s 725us/step - loss: 7.5062\n",
      "1/1 [==============================] - 0s 849us/step - loss: 7.7168\n",
      "1/1 [==============================] - 0s 955us/step - loss: 6.8547\n",
      "1/1 [==============================] - 0s 906us/step - loss: 6.7355\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 6.7330\n",
      "1/1 [==============================] - 0s 689us/step - loss: 6.4998\n",
      "1/1 [==============================] - 0s 895us/step - loss: 6.3402\n",
      "1/1 [==============================] - 0s 868us/step - loss: 6.3399\n",
      "1/1 [==============================] - 0s 999us/step - loss: 7.2657\n",
      "1/1 [==============================] - 0s 772us/step - loss: 6.7786\n",
      "1/1 [==============================] - 0s 769us/step - loss: 6.7698\n",
      "1/1 [==============================] - 0s 695us/step - loss: 6.9887\n",
      "1/1 [==============================] - 0s 842us/step - loss: 6.5224\n",
      "1/1 [==============================] - 0s 848us/step - loss: 6.8096\n",
      "1/1 [==============================] - 0s 700us/step - loss: 6.8053\n",
      "1/1 [==============================] - 0s 790us/step - loss: 7.1662\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 6.7995\n",
      "1/1 [==============================] - 0s 904us/step - loss: 6.7239\n",
      "1/1 [==============================] - 0s 798us/step - loss: 6.7910\n",
      "1/1 [==============================] - 0s 956us/step - loss: 6.6373\n",
      "1/1 [==============================] - 0s 704us/step - loss: 6.4593\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 6.6984\n",
      "1/1 [==============================] - 0s 742us/step - loss: 6.4327\n",
      "1/1 [==============================] - 0s 839us/step - loss: 6.2030\n",
      "1/1 [==============================] - 0s 752us/step - loss: 6.2772\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 6.6090\n",
      "1/1 [==============================] - 0s 963us/step - loss: 6.4906\n",
      "1/1 [==============================] - 0s 706us/step - loss: 6.6704\n",
      "1/1 [==============================] - 0s 983us/step - loss: 6.4348\n",
      "1/1 [==============================] - 0s 916us/step - loss: 6.5334\n",
      "1/1 [==============================] - 0s 719us/step - loss: 6.3862\n",
      "1/1 [==============================] - 0s 829us/step - loss: 6.5367\n",
      "1/1 [==============================] - 0s 693us/step - loss: 6.6643\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 6.5273\n",
      "1/1 [==============================] - 0s 825us/step - loss: 6.6495\n",
      "1/1 [==============================] - 0s 807us/step - loss: 6.7183\n",
      "1/1 [==============================] - 0s 762us/step - loss: 6.7036\n",
      "1/1 [==============================] - 0s 761us/step - loss: 6.4954\n",
      "1/1 [==============================] - 0s 760us/step - loss: 6.1331\n",
      "1/1 [==============================] - 0s 758us/step - loss: 6.3800\n",
      "1/1 [==============================] - 0s 767us/step - loss: 6.6736\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 6.3464\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 6.4800\n",
      "1/1 [==============================] - 0s 721us/step - loss: 6.2944\n",
      "1/1 [==============================] - 0s 956us/step - loss: 6.2578\n",
      "1/1 [==============================] - 0s 722us/step - loss: 6.3186\n",
      "1/1 [==============================] - 0s 705us/step - loss: 6.2747\n",
      "1/1 [==============================] - 0s 684us/step - loss: 6.3353\n",
      "1/1 [==============================] - 0s 840us/step - loss: 6.3003\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 6.1204\n",
      "1/1 [==============================] - 0s 977us/step - loss: 6.1433\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 5.9078\n",
      "1/1 [==============================] - 0s 768us/step - loss: 6.0204\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 6.7827\n",
      "1/1 [==============================] - 0s 685us/step - loss: 6.5984\n",
      "1/1 [==============================] - 0s 698us/step - loss: 6.9960\n",
      "1/1 [==============================] - 0s 739us/step - loss: 6.7657\n",
      "1/1 [==============================] - 0s 770us/step - loss: 6.4988\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 6.1895\n",
      "1/1 [==============================] - 0s 968us/step - loss: 6.4426\n",
      "1/1 [==============================] - 0s 875us/step - loss: 6.3486\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 6.3117\n",
      "1/1 [==============================] - 0s 690us/step - loss: 6.5715\n",
      "1/1 [==============================] - 0s 999us/step - loss: 6.3259\n",
      "1/1 [==============================] - 0s 701us/step - loss: 6.6971\n",
      "1/1 [==============================] - 0s 847us/step - loss: 6.4599\n",
      "1/1 [==============================] - 0s 708us/step - loss: 6.3123\n",
      "1/1 [==============================] - 0s 917us/step - loss: 6.6439\n",
      "1/1 [==============================] - 0s 994us/step - loss: 6.4540\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 6.9254\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 6.4714\n",
      "1/1 [==============================] - 0s 770us/step - loss: 6.4655\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 6.1299\n",
      "1/1 [==============================] - 0s 696us/step - loss: 6.3867\n",
      "1/1 [==============================] - 0s 952us/step - loss: 6.3151\n",
      "1/1 [==============================] - 0s 701us/step - loss: 6.5675\n",
      "1/1 [==============================] - 0s 988us/step - loss: 6.3120\n",
      "1/1 [==============================] - 0s 830us/step - loss: 6.1601\n",
      "1/1 [==============================] - 0s 721us/step - loss: 6.9423\n",
      "1/1 [==============================] - 0s 895us/step - loss: 6.1384\n",
      "1/1 [==============================] - 0s 723us/step - loss: 6.6856\n",
      "1/1 [==============================] - 0s 706us/step - loss: 6.8399\n",
      "1/1 [==============================] - 0s 706us/step - loss: 6.3003\n",
      "1/1 [==============================] - 0s 952us/step - loss: 6.2442\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 6.1933\n",
      "1/1 [==============================] - 0s 867us/step - loss: 6.2222\n",
      "1/1 [==============================] - 0s 778us/step - loss: 6.4698\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 790us/step - loss: 6.7151\n",
      "1/1 [==============================] - 0s 694us/step - loss: 6.5622\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 6.1982\n",
      "1/1 [==============================] - 0s 858us/step - loss: 6.3650\n",
      "1/1 [==============================] - 0s 718us/step - loss: 6.4184\n",
      "1/1 [==============================] - 0s 690us/step - loss: 6.8147\n",
      "1/1 [==============================] - 0s 708us/step - loss: 6.4119\n",
      "1/1 [==============================] - 0s 911us/step - loss: 6.3296\n",
      "1/1 [==============================] - 0s 690us/step - loss: 6.5182\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 6.2965\n",
      "1/1 [==============================] - 0s 870us/step - loss: 6.5603\n",
      "1/1 [==============================] - 0s 868us/step - loss: 6.3370\n",
      "1/1 [==============================] - 0s 868us/step - loss: 6.2300\n",
      "1/1 [==============================] - 0s 704us/step - loss: 6.2178\n",
      "1/1 [==============================] - 0s 715us/step - loss: 6.1267\n",
      "1/1 [==============================] - 0s 921us/step - loss: 6.0509\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 6.0953\n",
      "1/1 [==============================] - 0s 769us/step - loss: 6.1990\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 6.4402\n",
      "1/1 [==============================] - 0s 734us/step - loss: 6.4982\n",
      "1/1 [==============================] - 0s 998us/step - loss: 6.7922\n",
      "1/1 [==============================] - 0s 790us/step - loss: 6.5649\n",
      "1/1 [==============================] - 0s 693us/step - loss: 6.2501\n",
      "1/1 [==============================] - 0s 712us/step - loss: 6.8397\n",
      "1/1 [==============================] - 0s 719us/step - loss: 6.5247\n",
      "1/1 [==============================] - 0s 732us/step - loss: 7.0972\n",
      "1/1 [==============================] - 0s 720us/step - loss: 6.6724\n",
      "1/1 [==============================] - 0s 696us/step - loss: 6.7656\n",
      "1/1 [==============================] - 0s 705us/step - loss: 6.6042\n",
      "1/1 [==============================] - 0s 711us/step - loss: 6.5094\n",
      "1/1 [==============================] - 0s 770us/step - loss: 6.3969\n",
      "1/1 [==============================] - 0s 750us/step - loss: 6.2761\n",
      "1/1 [==============================] - 0s 863us/step - loss: 6.4047\n",
      "1/1 [==============================] - 0s 785us/step - loss: 6.7139\n",
      "1/1 [==============================] - 0s 761us/step - loss: 6.4721\n",
      "1/1 [==============================] - 0s 761us/step - loss: 6.3211\n",
      "1/1 [==============================] - 0s 772us/step - loss: 6.4234\n",
      "1/1 [==============================] - 0s 783us/step - loss: 6.4652\n",
      "1/1 [==============================] - 0s 759us/step - loss: 6.1306\n",
      "1/1 [==============================] - 0s 903us/step - loss: 6.5758\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 6.2343\n",
      "1/1 [==============================] - 0s 937us/step - loss: 6.8795\n",
      "1/1 [==============================] - 0s 779us/step - loss: 6.7150\n",
      "1/1 [==============================] - 0s 912us/step - loss: 6.4638\n",
      "1/1 [==============================] - 0s 765us/step - loss: 6.2268\n",
      "1/1 [==============================] - 0s 772us/step - loss: 6.7130\n",
      "1/1 [==============================] - 0s 765us/step - loss: 6.7843\n",
      "1/1 [==============================] - 0s 705us/step - loss: 6.3993\n",
      "1/1 [==============================] - 0s 879us/step - loss: 6.4002\n",
      "1/1 [==============================] - 0s 867us/step - loss: 6.3801\n",
      "1/1 [==============================] - 0s 736us/step - loss: 6.4543\n",
      "1/1 [==============================] - 0s 771us/step - loss: 6.4900\n",
      "1/1 [==============================] - 0s 959us/step - loss: 6.3962\n",
      "1/1 [==============================] - 0s 729us/step - loss: 6.1843\n",
      "1/1 [==============================] - 0s 932us/step - loss: 6.4212\n",
      "1/1 [==============================] - 0s 777us/step - loss: 6.4046\n",
      "1/1 [==============================] - 0s 799us/step - loss: 6.1812\n",
      "1/1 [==============================] - 0s 994us/step - loss: 6.2263\n",
      "1/1 [==============================] - 0s 721us/step - loss: 5.9605\n",
      "1/1 [==============================] - 0s 707us/step - loss: 6.0852\n",
      "1/1 [==============================] - 0s 901us/step - loss: 6.0606\n",
      "1/1 [==============================] - 0s 864us/step - loss: 6.1440\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 6.2562\n",
      "1/1 [==============================] - 0s 717us/step - loss: 7.2055\n",
      "1/1 [==============================] - 0s 874us/step - loss: 6.9988\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 6.8411\n",
      "1/1 [==============================] - 0s 954us/step - loss: 6.7827\n",
      "1/1 [==============================] - 0s 740us/step - loss: 6.6143\n",
      "1/1 [==============================] - 0s 694us/step - loss: 6.3553\n",
      "1/1 [==============================] - 0s 938us/step - loss: 6.7288\n",
      "1/1 [==============================] - 0s 956us/step - loss: 6.4204\n",
      "1/1 [==============================] - 0s 773us/step - loss: 6.3022\n",
      "1/1 [==============================] - 0s 750us/step - loss: 6.2233\n",
      "1/1 [==============================] - 0s 907us/step - loss: 6.2427\n",
      "1/1 [==============================] - 0s 766us/step - loss: 6.4784\n",
      "1/1 [==============================] - 0s 942us/step - loss: 6.2530\n",
      "1/1 [==============================] - 0s 971us/step - loss: 6.4745\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 6.2391\n",
      "1/1 [==============================] - 0s 922us/step - loss: 6.6303\n",
      "1/1 [==============================] - 0s 835us/step - loss: 6.6587\n",
      "1/1 [==============================] - 0s 940us/step - loss: 6.4649\n",
      "1/1 [==============================] - 0s 689us/step - loss: 6.6804\n",
      "1/1 [==============================] - 0s 951us/step - loss: 6.3668\n",
      "1/1 [==============================] - 0s 695us/step - loss: 6.9598\n",
      "1/1 [==============================] - 0s 720us/step - loss: 6.6409\n",
      "1/1 [==============================] - 0s 752us/step - loss: 6.6540\n",
      "1/1 [==============================] - 0s 949us/step - loss: 6.7495\n",
      "1/1 [==============================] - 0s 695us/step - loss: 6.6561\n",
      "1/1 [==============================] - 0s 865us/step - loss: 6.6449\n",
      "1/1 [==============================] - 0s 823us/step - loss: 6.5783\n",
      "1/1 [==============================] - 0s 692us/step - loss: 6.4512\n",
      "1/1 [==============================] - 0s 731us/step - loss: 6.4995\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 6.6837\n",
      "1/1 [==============================] - 0s 866us/step - loss: 6.4834\n",
      "1/1 [==============================] - 0s 700us/step - loss: 6.4418\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 6.3012\n",
      "1/1 [==============================] - 0s 795us/step - loss: 6.3117\n",
      "1/1 [==============================] - 0s 720us/step - loss: 6.3372\n",
      "1/1 [==============================] - 0s 691us/step - loss: 6.5435\n",
      "1/1 [==============================] - 0s 768us/step - loss: 6.3400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 6.3385\n",
      "1/1 [==============================] - 0s 788us/step - loss: 6.2353\n",
      "1/1 [==============================] - 0s 910us/step - loss: 6.2843\n",
      "1/1 [==============================] - 0s 853us/step - loss: 6.2909\n",
      "1/1 [==============================] - 0s 749us/step - loss: 6.3531\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 6.1770\n",
      "1/1 [==============================] - 0s 710us/step - loss: 6.4468\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 6.6405\n",
      "1/1 [==============================] - 0s 871us/step - loss: 6.6249\n",
      "1/1 [==============================] - 0s 873us/step - loss: 6.8747\n",
      "1/1 [==============================] - 0s 689us/step - loss: 6.5078\n",
      "1/1 [==============================] - 0s 744us/step - loss: 6.6818\n",
      "1/1 [==============================] - 0s 890us/step - loss: 6.5032\n",
      "1/1 [==============================] - 0s 708us/step - loss: 6.7157\n",
      "1/1 [==============================] - 0s 919us/step - loss: 6.3026\n",
      "1/1 [==============================] - 0s 877us/step - loss: 6.2609\n",
      "1/1 [==============================] - 0s 744us/step - loss: 6.2483\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-d4e5d834d795>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mpbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_gen\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mhist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarg\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mpbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_postfix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mpbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/1-workdir/nlp_attention/en_vi_attention_nlp/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/1-workdir/nlp_attention/en_vi_attention_nlp/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    846\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m    847\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 848\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    849\u001b[0m               \u001b[0;31m# Catch OutOfRangeError for Datasets of unknown size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m               \u001b[0;31m# This blocks until the batch has finished executing.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/1-workdir/nlp_attention/en_vi_attention_nlp/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/1-workdir/nlp_attention/en_vi_attention_nlp/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    609\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    612\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/1-workdir/nlp_attention/en_vi_attention_nlp/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2418\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2419\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2420\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2422\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/1-workdir/nlp_attention/en_vi_attention_nlp/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1663\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1664\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1665\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1666\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1667\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/1-workdir/nlp_attention/en_vi_attention_nlp/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1744\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1746\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1748\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/1-workdir/nlp_attention/en_vi_attention_nlp/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    596\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    599\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/1-workdir/nlp_attention/en_vi_attention_nlp/lib/python3.6/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "EPOCHS = 15\n",
    "pbar = tqdm(total=N_STEPS_PER_EPOCH_TRAIN)\n",
    "for epoch in range(EPOCHS):\n",
    "    i = 0\n",
    "    pbar.reset()\n",
    "    for inp, targ in train_gen:\n",
    "        hist = training_model.fit([inp, targ])\n",
    "        pbar.set_postfix(cost = hist.history['loss'][-1])\n",
    "        pbar.update(1)\n",
    "        i = i+1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
    }
   ],
   "source": [
    "physical_devices = tf.config.experimental.list_physical_devices('GPU') \n",
    "for physical_device in physical_devices: \n",
    "    tf.config.experimental.set_memory_growth(physical_device, True)\n",
    "\n",
    "print(physical_devices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "/home/sweet/1-workdir/nlp_attention/en_vi_attention_nlp\n"
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_dir = \"/home/sweet/1-workdir/nlp_attention/en_vi_data_preprocess/src/\"\n",
    "db_file = \"train-test.json\"\n",
    "dict_file = \"dictionary.json\"\n",
    "\n",
    "train_X = []\n",
    "train_Y = []\n",
    "\n",
    "test_X = []\n",
    "test_Y = []\n",
    "\n",
    "with open(db_dir + db_file, 'r') as f_db, open(db_dir + dict_file, 'r') as f_dict:\n",
    "    db = json.load(f_db)\n",
    "    dictionary = json.load(f_dict)\n",
    "    \n",
    "train_X = db['train_X']\n",
    "train_Y = db['train_Y']\n",
    "test_X = db['test_X']\n",
    "test_Y = db['test_Y']\n",
    "\n",
    "dictionary_from = dictionary['from']['dictionary']\n",
    "rev_dictionary_from = dictionary['from']['rev_dictionary']\n",
    "\n",
    "dictionary_to = dictionary['to']['dictionary']\n",
    "rev_dictionary_to = dictionary['to']['rev_dictionary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "GO = dictionary_from['GO']\n",
    "PAD = dictionary_from['PAD']\n",
    "EOS = dictionary_from['EOS']\n",
    "UNK = dictionary_from['UNK']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "100%|██████████| 133317/133317 [00:00<00:00, 2354676.22it/s]\n"
    }
   ],
   "source": [
    "for i in tqdm(range(len(train_X))):\n",
    "    train_X[i] += ' EOS'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "100%|██████████| 2821/2821 [00:00<00:00, 1161151.28it/s]\n"
    }
   ],
   "source": [
    "for i in tqdm(range(len(test_X))):\n",
    "    test_X[i] += ' EOS'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "48114 22468\n"
    }
   ],
   "source": [
    "num_encoders=6\n",
    "num_multi_heads=8\n",
    "d_k=64\n",
    "d_v=64\n",
    "d_model=512\n",
    "optimizer=\"adam\"\n",
    "null_token_value=0\n",
    "source_vocab_size = len(dictionary_from)\n",
    "target_vocab_size = len(dictionary_to)\n",
    "share_word_embedding=False\n",
    "MAXIMUM_TEXT_LENGTH = 250\n",
    "\n",
    "print(source_vocab_size, target_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def str_idx(corpus, dic):\n",
    "    X = []\n",
    "    for i in corpus:\n",
    "        ints = []\n",
    "        for k in i.split():\n",
    "            ints.append(dic.get(k,UNK))\n",
    "        X.append(ints)\n",
    "    return X\n",
    "\n",
    "def pad_sentence_batch(sentence_batch, pad_int):\n",
    "    padded_seqs = []\n",
    "    seq_lens = []\n",
    "    max_sentence_len = max([len(sentence) for sentence in sentence_batch])\n",
    "    for sentence in sentence_batch:\n",
    "        padded_seqs.append(sentence + [pad_int] * (max_sentence_len - len(sentence)))\n",
    "        seq_lens.append(len(sentence))\n",
    "    return np.array(padded_seqs, dtype=np.int32), np.array(seq_lens)\n",
    "\n",
    "def pad_along_axis(array: np.ndarray, target_length: int, axis: int = 0):\n",
    "    pad_size = target_length - array.shape[axis]\n",
    "    if pad_size <= 0:\n",
    "        return array\n",
    "    npad = [(0, 0)] * array.ndim\n",
    "    npad[axis] = (0, pad_size)\n",
    "    return np.pad(array, pad_width=npad, mode='constant', constant_values=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = str_idx(train_X, dictionary_from)\n",
    "test_X = str_idx(test_X, dictionary_from)\n",
    "train_Y = str_idx(train_Y, dictionary_to)\n",
    "test_Y = str_idx(test_Y, dictionary_to)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(133317, 685) (133317, 865)\n"
    }
   ],
   "source": [
    "padded_train_X, _ = pad_sentence_batch(train_X, PAD)\n",
    "padded_train_Y, _ = pad_sentence_batch(train_Y, PAD)\n",
    "print(padded_train_X.shape, padded_train_Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(2821, 104) (2821, 123)\n"
    }
   ],
   "source": [
    "padded_test_X, _ = pad_sentence_batch(test_X, PAD)\n",
    "padded_test_Y, _ = pad_sentence_batch(test_Y, PAD)\n",
    "print(padded_test_X.shape, padded_test_Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(133317, 865) (133317, 865)\n(2821, 123) (2821, 123)\n"
    }
   ],
   "source": [
    "max_char_length = max(padded_train_X.shape[1], padded_train_Y.shape[1])\n",
    "padded_train_X = pad_along_axis(padded_train_X, max_char_length, axis=1)\n",
    "padded_train_Y = pad_along_axis(padded_train_Y, max_char_length, axis=1)\n",
    "print(padded_train_X.shape, padded_train_Y.shape)\n",
    "\n",
    "max_char_length = max(padded_test_X.shape[1], padded_test_Y.shape[1])\n",
    "padded_test_X = pad_along_axis(padded_test_X, max_char_length, axis=1)\n",
    "padded_test_Y = pad_along_axis(padded_test_Y, max_char_length, axis=1)\n",
    "print(padded_test_X.shape, padded_test_Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _bytes_feature(value):\n",
    "    \"\"\"Returns a bytes_list from a string / byte.\"\"\"\n",
    "    if isinstance(value, type(tf.constant(0))):\n",
    "        value = value.numpy() # BytesList won't unpack a string from an EagerTensor.\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "def serialize_array(array):\n",
    "    array = tf.io.serialize_tensor(array)\n",
    "    return array\n",
    "\n",
    "def image_example(input_arr, target_arr):\n",
    "\n",
    "    feature = {\n",
    "      'input': _bytes_feature(input_arr),\n",
    "      'target': _bytes_feature(target_arr),\n",
    "    }\n",
    "\n",
    "    return tf.train.Example(features=tf.train.Features(feature=feature))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "100%|██████████| 133317/133317 [00:16<00:00, 8159.88it/s]\n"
    }
   ],
   "source": [
    "record_file = 'train.tfrecords'\n",
    "with tf.io.TFRecordWriter(record_file) as writer:\n",
    "    for i in tqdm(range(len(padded_train_X))):\n",
    "        tf_example = image_example(serialize_array(padded_train_X[i]), serialize_array(padded_train_Y[i]))\n",
    "        writer.write(tf_example.SerializeToString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "100%|██████████| 2821/2821 [00:00<00:00, 9347.89it/s]\n"
    }
   ],
   "source": [
    "record_file = 'test.tfrecords'\n",
    "with tf.io.TFRecordWriter(record_file) as writer:\n",
    "    for i in tqdm(range(len(padded_test_X))):\n",
    "        tf_example = image_example(serialize_array(padded_test_X[i]), serialize_array(padded_test_Y[i]))\n",
    "        writer.write(tf_example.SerializeToString())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}